{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.22621}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs22\lang9 # Packages for EDA\par
\par
import matplotlib.pyplot as plt\par
import seaborn as sns\par
import pandas as pd\par
\par
\par
# for plots to be displayed directly in the notebook\par
%matplotlib inline\par
\par
# the visual style for aesthetics\par
sns.set(color_codes=True)\par
\par
\par
# Loaded the client data and pricing data\par
\par
client_df = pd.read_csv("C:\\\\Users\\\\sanoj\\\\Desktop\\\\Data Science\\\\BCG task 2\\\\client_data (1).csv")\par
price_df = pd.read_csv("C:\\\\Users\\\\sanoj\\\\Desktop\\\\Data Science\\\\BCG task 2\\\\price_data (1).csv")\par
\par
\par
\par
# to display the first 7 rows of the client data to get an overview\par
client_df.head(7)\par
\par
\par
\par
# to display the first 7 rows of the price data to get an overview\par
price_df.head(7)\par
\par
# Exploring the data types and basic statistics to understand the structure\par
client_df.info()\par
client_df.describe()\par
\par
\par
# Exploring the data types and basic statistics to understand the structure\par
price_df.info()\par
price_df.describe()\par
\par
# Extracted relevant columns related to energy consumption for further analysis\par
price_columns = ['id', 'price_date', 'price_off_peak_var', 'price_peak_var', 'price_mid_peak_var',\par
                  'price_off_peak_fix', 'price_peak_fix', 'price_mid_peak_fix']\par
\par
price = price_df[price_columns]\par
\par
# Displayed the first 7 rows of the price data to understand its structure\par
price.head(7)\par
\par
# Calculate churn percentages to understand the overall churn status\par
churn_total = churn.groupby(churn['churn']).count()\par
churn_percentage = churn_total / churn_total.sum() * 100\par
\par
# Map churn labels for clearer visualization\par
churn_labels = \{1: 'Churn', 0: 'Retention'\}\par
churn.loc[:, 'churn_label'] = churn['churn'].map(churn_labels)\par
\par
# Visualize the churn status using multiple visualizations for a comprehensive view\par
\par
# pie chart to show the proportion of churned and retained customers\par
plt.figure(figsize=(10, 6))\par
plt.pie(churn_percentage['id'], labels=churn_percentage.index.map(churn_labels), autopct='%1.1f%%', startangle=90, colors=['#FF9999', '#66B2FF'])\par
plt.title('Churning Status Proportion')\par
plt.show()\par
\par
# bar chart for a clearer representation\par
plt.figure(figsize=(8, 5))\par
sns.barplot(x=churn_percentage.index.map(churn_labels), y=churn_percentage['id'], palette=['#FF9999', '#66B2FF'])\par
plt.title('Churning Status Distribution')\par
plt.xlabel('Churn Status')\par
plt.ylabel('Percentage')\par
plt.show()\par
\par
# the count of churn status using a count plot\par
plt.figure(figsize=(8, 5))\par
sns.countplot(x='churn_label', data=churn, palette=['#FF9999', '#66B2FF'])\par
plt.title('Count of Churn Status')\par
plt.xlabel('Churn Status')\par
plt.ylabel('Count')\par
plt.show()\par
\par
# summary of churn statistics\par
print(f"Churn Statistics:\\n\{churn_percentage.transpose()\}")\par
\par
# Additional insights or interpretations\par
print("\\nAdditional Insights:")\par
print("- The pie chart provides a visual breakdown of churn proportions.")\par
print("- The bar chart offers a clear comparison of churn percentages.")\par
print("- The count plot shows the actual count of churned and retained customers.")\par
\par
#All these plots clearly show that nearly 10% of subscribers are churning, \par
#and we need to further investigate to understand what affects churning.\par
\par
\par
# Select relevant columns for energy consumption analysis\par
consumption_columns = ['id', 'cons_12m', 'cons_gas_12m', 'cons_last_month', 'imp_cons', 'has_gas', 'churn']\par
consumption = client_df[consumption_columns].copy()  # Create a copy of the DataFrame\par
\par
# Mapping labels for better readability\par
churn_labels = \{1: 'Churn', 0: 'Retention'\}\par
consumption['churn_label'] = consumption['churn'].map(churn_labels)\par
\par
# Creating subplots for distribution plots to understand the spread of 'cons_12m'\par
fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(18, 10))\par
\par
# Plot the distribution of 'cons_12m' with a histogram\par
sns.histplot(consumption['cons_12m'], bins=50, kde=True, color='#66B2FF', ax=axs[0, 0])\par
axs[0, 0].set_title('Distribution of Annual Consumption')\par
axs[0, 0].set_xlabel('Annual Consumption (kWh)')\par
axs[0, 0].set_ylabel('Frequency')\par
\par
# Explore the relationship between gas consumption and churn\par
sns.scatterplot(x='cons_gas_12m', y='cons_12m', hue='churn_label', data=consumption, palette=['#66B2FF', '#FF9999'], ax=axs[0, 1])\par
axs[0, 1].set_title('Gas vs. Electricity Consumption')\par
axs[0, 1].set_xlabel('Gas Consumption (kWh)')\par
axs[0, 1].set_ylabel('Electricity Consumption (kWh)')\par
\par
# Investigate the impact of gas usage on churn\par
sns.boxplot(x='churn_label', y='cons_gas_12m', data=consumption, palette=['#FF9999', '#66B2FF'], ax=axs[1, 0])\par
axs[1, 0].set_title('Impact of Gas Usage on Churn')\par
axs[1, 0].set_xlabel('Churn Status')\par
axs[1, 0].set_ylabel('Gas Consumption (kWh)')\par
\par
# Analyze the distribution of the last month's consumption\par
sns.kdeplot(consumption.loc[consumption['churn_label'] == 'Retention', 'cons_last_month'], label='Retention', color='#66B2FF', ax=axs[1, 1])\par
sns.kdeplot(consumption.loc[consumption['churn_label'] == 'Churn', 'cons_last_month'], label='Churn', color='#FF9999', ax=axs[1, 1])\par
axs[1, 1].set_title('Distribution of Last Month\\'s Consumption')\par
axs[1, 1].set_xlabel('Last Month\\'s Consumption (kWh)')\par
axs[1, 1].set_ylabel('Density')\par
\par
# Adjust layout for better readability\par
plt.tight_layout()\par
plt.show()\par
\par
# Additional insights or interpretations can be added here\par
print("\\nAdditional Insights:")\par
print("- The first subplot displays the distribution of annual consumption.")\par
print("- The scatter plot examines the relationship between gas and electricity consumption, color-coded by churn status.")\par
print("- The second row explores the impact of gas usage on churn through a box plot.")\par
print("- The last subplot compares the distribution of last month's consumption for retained and churned customers.")\par
\par
\par
# Convert date columns to datetime format\par
client_df['date_activ'] = pd.to_datetime(client_df['date_activ'])\par
client_df['date_end'] = pd.to_datetime(client_df['date_end'])\par
client_df['date_modif_prod'] = pd.to_datetime(client_df['date_modif_prod'])\par
client_df['date_renewal'] = pd.to_datetime(client_df['date_renewal'])\par
\par
# Extract features related to date\par
client_df['activation_month'] = client_df['date_activ'].dt.month\par
client_df['activation_year'] = client_df['date_activ'].dt.year\par
client_df['end_month'] = client_df['date_end'].dt.month\par
client_df['end_year'] = client_df['date_end'].dt.year\par
client_df['modification_month'] = client_df['date_modif_prod'].dt.month\par
client_df['modification_year'] = client_df['date_modif_prod'].dt.year\par
client_df['renewal_month'] = client_df['date_renewal'].dt.month\par
client_df['renewal_year'] = client_df['date_renewal'].dt.year\par
\par
# Visualize churn over time\par
plt.figure(figsize=(12, 6))\par
sns.lineplot(x='activation_year', y='churn', data=client_df, marker='o')\par
plt.title('Churn Over Time')\par
plt.xlabel('Activation Year')\par
plt.ylabel('Churn Rate')\par
plt.show()\par
\par
# Investigate seasonal patterns\par
plt.figure(figsize=(12, 6))\par
sns.lineplot(x='activation_month', y='churn', data=client_df, marker='o')\par
plt.title('Seasonal Churn Patterns')\par
plt.xlabel('Activation Month')\par
plt.ylabel('Churn Rate')\par
plt.show()\par
\par
# Explore correlations with numerical features\par
numerical_columns = ['cons_12m', 'cons_gas_12m', 'cons_last_month', 'imp_cons', 'net_margin', 'num_years_antig', 'pow_max']\par
correlation_matrix = client_df[numerical_columns + ['churn']].corr()\par
\par
plt.figure(figsize=(10, 8))\par
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")\par
plt.title('Correlation Matrix')\par
plt.show()\par
\par
\par
# Churn Analysis Results:\par
\par
# Churn Rate:\par
# The churn rate is 9.7%, indicating that approximately 9.7% of subscribers have churned, while 90.3% are retained.\par
\par
# Electricity Consumption Impact:\par
# Churn frequency was higher among customers with lower gas consumption, particularly near 0 kWh.\par
# Electricity consumption might have a more significant impact on churn compared to gas consumption.\par
\par
# Temporal Trends:\par
# Churn rate was lowest in the year 2007 and highest in 2013, suggesting temporal variations in churn behavior.\par
# Churn rate was lowest in September and highest in June, indicating potential seasonality.\par
\par
# Correlation Analysis:\par
# There is a positive correlation of 0.46 between total net margin and current paid consumption, suggesting a moderate relationship between net margin and consumption.\par
# The correlation between electricity consumption of the last month and  gas consumption of the past 12 months is 0.51, indicating a moderate positive relationship.\par
# The correlation between gas consumption of the past 12 months and electricity consumption of the past 12 months is 0.49, suggesting a moderate positive correlation.\par
\par
# Conclusion:\par
# The findings highlight the importance of considering electricity consumption, temporal trends, and correlations with key features like net margin and consumption patterns in understanding and predicting churn behavior.\par
\par
# These insights provide a foundation for further exploration and targeted strategies to mitigate churn and enhance customer retention.\par
\par
# Feature Engineering\par
\par
---\par
\par
1. Import packages\par
2. Load data\par
3. Feature engineering\par
\par
---\par
\par
## 1. Import packages\par
\par
import pandas as pd\par
\par
---\par
## 2. Load data\par
\par
df = pd.read_csv("C:\\\\Users\\\\sanoj\\\\Desktop\\\\Data Science\\\\BCG task\\\\clean_data_after_eda.csv")\par
df["date_activ"] = pd.to_datetime(df["date_activ"], format='%Y-%m-%d')\par
df["date_end"] = pd.to_datetime(df["date_end"], format='%Y-%m-%d')\par
df["date_modif_prod"] = pd.to_datetime(df["date_modif_prod"], format='%Y-%m-%d')\par
df["date_renewal"] = pd.to_datetime(df["date_renewal"], format='%Y-%m-%d')\par
\par
df.head(3)\par
\par
---\par
\par
## 3. Feature engineering\par
\par
### Difference between off-peak prices in December and preceding January\par
\par
Below is the code created by your colleague to calculate the feature described above. Use this code to re-create this feature and then think about ways to build on this feature to create features with a higher predictive power.\par
\par
price_df = pd.read_csv("C:\\\\Users\\\\sanoj\\\\Desktop\\\\Data Science\\\\BCG task\\\\price_data (1).csv")\par
price_df["price_date"] = pd.to_datetime(price_df["price_date"], format='%Y-%m-%d')\par
price_df.head()\par
\par
# Group off-peak prices by companies and month\par
monthly_price_by_id = price_df.groupby(['id', 'price_date']).agg(\{'price_off_peak_var': 'mean', 'price_off_peak_fix': 'mean'\}).reset_index()\par
\par
# Get january and december prices\par
jan_prices = monthly_price_by_id.groupby('id').first().reset_index()\par
dec_prices = monthly_price_by_id.groupby('id').last().reset_index()\par
\par
# Calculate the difference\par
diff = pd.merge(dec_prices.rename(columns=\{'price_off_peak_var': 'dec_1', 'price_off_peak_fix': 'dec_2'\}), jan_prices.drop(columns='price_date'), on='id')\par
diff['offpeak_diff_dec_january_energy'] = diff['dec_1'] - diff['price_off_peak_var']\par
diff['offpeak_diff_dec_january_power'] = diff['dec_2'] - diff['price_off_peak_fix']\par
diff = diff[['id', 'offpeak_diff_dec_january_energy','offpeak_diff_dec_january_power']]\par
diff.head()\par
\par
Now it is time to get creative and to conduct some of your own feature engineering! Have fun with it, explore different ideas and try to create as many as yo can!\par
\par
Time based Features\par
\par
Extracting month, day of the month, day of the year, and year from date columns can provide additional time-related features.\par
\par
\par
\par
df['month_activ'] = df['date_activ'].dt.month\par
df['day_activ'] = df['date_activ'].dt.day\par
df['year_activ'] = df['date_activ'].dt.year\par
df.head (5)\par
\par
\par
Time Differences\par
\par
Calculate the differences between various date columns, which might capture patterns related to customer behavior.\par
\par
df['days_until_end'] = (df['date_end'] - df['date_activ']).dt.days\par
df['days_until_renewal'] = (df['date_renewal'] - df['date_activ']).dt.days\par
df.head(5)\par
\par
\par
Consumption Statistics:\par
Derive statistical features such as the mean and standard deviation from numerical columns like consumption, providing insights into data variability.\par
\par
df['mean_consumption'] = df[['cons_12m', 'cons_gas_12m', 'cons_last_month']].mean(axis=1)\par
df['std_consumption'] = df[['cons_12m', 'cons_gas_12m', 'cons_last_month']].std(axis=1)\par
df.head(5)\par
\par
\par
Rolling Averages:\par
Calculate rolling averages of prices to capture trends over time.\par
\par
price_df['rolling_avg_var'] = price_df.groupby('id')['price_off_peak_var'].rolling(window=3).mean().reset_index(level=0, drop=True)\par
price_df.head(5)\par
\par
Temporal Trends:\par
Compute rolling averages of prices to reveal temporal trends and fluctuations.\par
\par
price_df['rolling_avg_var'] = price_df.groupby('id')['price_off_peak_var'].rolling(window=3).mean().reset_index(level=0, drop=True)\par
price_df.head(5)\par
\par
Aggregate by Customer ID:\par
Calculate aggregate statistics for each customer based on historical data.\par
\par
customer_agg = df.groupby('id').agg(\{'cons_12m': ['mean', 'max'], 'forecast_price_energy_off_peak': 'mean'\})\par
customer_agg.columns = ['avg_cons_12m', 'max_cons_12m', 'avg_price_off_peak_var']\par
df.head(5)\par
\par
\par
\par
Product and Ratio:\par
Introduce new features by performing mathematical operations on existing columns, like multiplication or division.\par
\par
df['product_consumption'] = df['cons_12m'] * df['cons_gas_12m']\par
df['ratio_consumption'] = df['cons_12m'] / (df['cons_gas_12m'] + 1)  # Avoid division by zero\par
df.head()\par
\par
\par
Energy Efficiency Metric:\par
Create a feature that represents the energy efficiency of each customer, offering a domain-specific perspective.\par
\par
df['energy_efficiency_electricity'] = df['cons_12m'] / df['num_years_antig']\par
df['energy_efficiency_gas'] = df['cons_gas_12m'] / df['num_years_antig']\par
df.head(5)\par
# Feature Engineering and Modelling\par
\par
---\par
\par
1. Import packages\par
2. Load data\par
3. Modelling\par
\par
---\par
\par
## 1. Import packages\par
\par
import warnings\par
warnings.filterwarnings("ignore", category=FutureWarning)\par
\par
import pandas as pd\par
import numpy as np\par
import seaborn as sns\par
from datetime import datetime\par
import matplotlib.pyplot as plt\par
\par
# Shows plots in jupyter notebook\par
%matplotlib inline\par
\par
# Set plot style\par
sns.set(color_codes=True)\par
\par
---\par
## 2. Load data\par
\par
df = pd.read_csv("C:\\\\Users\\\\sanoj\\\\Desktop\\\\Data Science\\\\BCG task\\\\data_for_predictions.csv")\par
df.drop(columns=["Unnamed: 0"], inplace=True)\par
df.head()\par
\par
---\par
\par
## 3. Modelling\par
\par
We now have a dataset containing features that we have engineered and we are ready to start training a predictive model. Remember, we only need to focus on training a `Random Forest` classifier.\par
\par
from sklearn import metrics\par
from sklearn.model_selection import train_test_split\par
from sklearn.ensemble import RandomForestClassifier\par
\par
### Data sampling\par
\par
The first thing we want to do is split our dataset into training and test samples. The reason why we do this, is so that we can simulate a real life situation by generating predictions for our test sample, without showing the predictive model these data points. This gives us the ability to see how well our model is able to generalise to new data, which is critical.\par
\par
A typical % to dedicate to testing is between 20-30, for this example we will use a 75-25% split between train and test respectively.\par
\par
# Make a copy of our data\par
train_df = df.copy()\par
\par
# Separate target variable from independent variables\par
y = df['churn']\par
X = df.drop(columns=['id', 'churn'])\par
print(X.shape)\par
print(y.shape)\par
\par
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\par
print(X_train.shape)\par
print(y_train.shape)\par
print(X_test.shape)\par
print(y_test.shape)\par
\par
### Model training\par
\par
Once again, we are using a `Random Forest` classifier in this example. A Random Forest sits within the category of `ensemble` algorithms because internally the `Forest` refers to a collection of `Decision Trees` which are tree-based learning algorithms. As the data scientist, you can control how large the forest is (that is, how many decision trees you want to include).\par
\par
The reason why an `ensemble` algorithm is powerful is because of the laws of averaging, weak learners and the central limit theorem. If we take a single decision tree and give it a sample of data and some parameters, it will learn patterns from the data. It may be overfit or it may be underfit, but that is now our only hope, that single algorithm. \par
\par
With `ensemble` methods, instead of banking on 1 single trained model, we can train 1000's of decision trees, all using different splits of the data and learning different patterns. It would be like asking 1000 people to all learn how to code. You would end up with 1000 people with different answers, methods and styles! The weak learner notion applies here too, it has been found that if you train your learners not to overfit, but to learn weak patterns within the data and you have a lot of these weak learners, together they come together to form a highly predictive pool of knowledge! This is a real life application of many brains are better than 1.\par
\par
Now instead of relying on 1 single decision tree for prediction, the random forest puts it to the overall views of the entire collection of decision trees. Some ensemble algorithms using a voting approach to decide which prediction is best, others using averaging. \par
\par
As we increase the number of learners, the idea is that the random forest's performance should converge to its best possible solution.\par
\par
Some additional advantages of the random forest classifier include:\par
\par
- The random forest uses a rule-based approach instead of a distance calculation and so features do not need to be scaled\par
- It is able to handle non-linear parameters better than linear based models\par
\par
On the flip side, some disadvantages of the random forest classifier include:\par
\par
- The computational power needed to train a random forest on a large dataset is high, since we need to build a whole ensemble of estimators.\par
- Training time can be longer due to the increased complexity and size of thee ensemble\par
\par
# Add model training in here!\par
model = RandomForestClassifier(random_state=42) # Add parameters to the model!\par
model.fit(X_train, y_train)\par
\par
### Evaluation\par
\par
Now let's evaluate how well this trained model is able to predict the values of the test dataset.\par
\par
y_pred = model.predict(X_test)\par
accuracy = metrics.accuracy_score(y_test, y_pred)\par
precision = metrics.precision_score(y_test, y_pred)\par
recall = metrics.recall_score(y_test, y_pred)\par
f1_score = metrics.f1_score(y_test, y_pred)\par
\par
conf_matrix = metrics.confusion_matrix(y_test, y_pred)\par
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="PuRd", cbar=False)\par
plt.xlabel("Predicted")\par
plt.ylabel("Actual")\par
plt.title("Confusion Matrix")\par
plt.show()\par
\par
# Confusion Matrix Analysis\par
"""\par
The confusion matrix provides insight into the model's performance:\par
\par
True Negatives (TN): 3282 - The number of instances correctly predicted as non-churn (actual 0, predicted 0).\par
True Positives (TP): 19 - The number of instances correctly predicted as churn (actual 1, predicted 1).\par
False Negatives (FN): 347 - Instances where the model predicted non-churn, but the actual label was churn (actual 1, predicted 0).\par
False Positives (FP): 4 - Instances where the model predicted churn, but the actual label was non-churn (actual 0, predicted 1).\par
\par
High True Negatives (3282) indicate a strong ability to correctly identify non-churn cases.\par
True Positives (19) show the model's capability in predicting churn instances.\par
False Negatives (347) represent instances where the model failed to identify actual churn cases.\par
False Positives (4) indicate instances where the model incorrectly predicted churn.\par
\par
Business Implications:\par
1) High True Negatives suggest that the model is effective in retaining customers who are unlikely to churn, minimizing the risk of unnecessary intervention for loyal customers.\par
2) True Positives highlight instances where the model successfully predicts churn, providing an opportunity for proactive retention strategies.\par
3) False Negatives represent missed opportunities to identify customers at risk of churning, signaling a need for enhanced targeted retention efforts.\par
4) False Positives indicate instances where intervention may be wrongly triggered, potentially leading to unnecessary costs. This underscores the importance of refining the model's threshold or implementing additional validation steps.\par
\par
The choice of evaluation metrics (accuracy, precision, recall, and F1 score) allows for a comprehensive assessment of the model's performance, balancing the impact of false positives and false negatives on the business. Striking the right balance is essential based on the specific goals and consequences of the churn prediction problem.\par
"""\par
\par
# Feature importance plot\par
feature_importance = model.feature_importances_\par
feature_names = X.columns\par
\par
# Sort feature importances in descending order\par
indices = np.argsort(feature_importance)[::-1]\par
\par
# Plot the feature importances\par
plt.figure(figsize=(10, 6))\par
sns.barplot(x=feature_importance[indices], y=feature_names[indices])\par
plt.xlabel('Feature Importance')\par
plt.ylabel('Features')\par
plt.title('Random Forest Classifier - Feature Importance')\par
plt.show()\par
\par
# Feature Importance Analysis\par
"""\par
The feature importance plot reveals insights into the factors influencing the Random Forest model's predictions. Notably, the feature with the highest importance is "electricity consumption of the past 12 months."\par
\par
Interpretation:\par
1) "Electricity consumption of the past 12 months" has the highest feature importance, suggesting that it significantly contributes to the model's prediction of churn.\par
2) High feature importance indicates that variations in this feature have a substantial impact on the model's decision-making process.\par
\par
Possible Business Implications:\par
1) The prominence of "electricity consumption" suggests that energy usage patterns over the past year play a crucial role in predicting customer churn.\par
2) Businesses may want to explore how electricity consumption patterns correlate with customer behavior and satisfaction.\par
3) Strategies focused on customer engagement, satisfaction, or tailored retention plans could be developed based on insights derived from this key feature.\par
\par
It's important to note that while "electricity consumption" is the most influential feature, other features in the dataset may also contribute to the model's performance, and a holistic understanding of various factors is necessary for comprehensive insights.\par
"""\par
}
 